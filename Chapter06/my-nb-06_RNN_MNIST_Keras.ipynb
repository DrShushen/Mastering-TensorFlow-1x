{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for MNIST with TensorFlow and Keras\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview Notes\n",
    "\n",
    "In problems involving ordered sequences of data, such as **time series Forecasting** and **natural language processing**, the context is very valuable to predict the output. The context for such problems can be determined by ingesting the whole sequence, not just one last data point. Thus, the previous output becomes part of the current input, and when repeated, the last output turns out to be the results of all the previous inputs along with the last input.\n",
    "\n",
    "**Recurrent Neural Network (RNN)** architecture is a solution for handling machine learning problems that involve sequences. RNN is a specialized neural network architecture for handling sequential data. The sequential data could be the sequence of observations over a period of time, as in time series data, or sequence of characters, words, and sentences, as in textual data.\n",
    "\n",
    "One of the assumptions for the *standard* neural network is that the input data is arranged in a way that *one input has no dependency on another*. However, for time series data and textual data, this assumption **does not hold true**, since the values appearing later in the sequence are often influenced by the values that appeared before.\n",
    "\n",
    "In order to achieve that, RNN extends the standard neural networks in the following ways:\n",
    "* RNN adds the ability to use the output of one layer as an input to the same or previous layer, by adding *loops or cycles* in the computation graph.\n",
    "* RNN adds **the memory unit** to store previous inputs and outputs that can be used in the current computation.\n",
    "\n",
    "In this chapter, we cover the following topics to learn about RNN:\n",
    "* Simple Recurrent Neural Networks\n",
    "\n",
    "* RNN variants\n",
    "    * Long Short-Term Memory networks (LSTM)\n",
    "    * Gated Recurrent Unit networks (GRU)\n",
    "\n",
    "\n",
    "* TensorFlow for RNN\n",
    "* Keras for RNN\n",
    "* RNN in Keras for MNIST data\n",
    "\n",
    "#### **\\[üìö\\] For detailed discussion and computational graphs of RNN, LSTM, GRU, see Book, Chapter 6.**\n",
    "\n",
    "### Workflow:\n",
    "The basic workflow for creating RNN models in low-level TensorFlow library is almost the same as MLP:\n",
    "1. First create the input and output placeholders of shape `(None, #TimeSteps, #Features)` or `(BatchSize, #TimeSteps, #Features)`\n",
    "2. From the input placeholder, create a list of length `#TimeSteps`, containing Tensors of Shape `(None, #Features)` or `(Batch Size, #Features)`\n",
    "3. Create a cell of the desired RNN type from the `tf.rnn.rnn_cell` module\n",
    "4. Use the cell and the input tensor list created previously to create a static or dynamic RNN\n",
    "5. Create the output weights and bias variables, and define the loss and optimizer functions\n",
    "6. For the required number of epochs, train the model using the loss and optimizer functions\n",
    "\n",
    "This **basic workflow** would be demonstrated with the example code in the **next chapter**.\n",
    "\n",
    "#### **\\[üìö\\] For details on building-block classes available in TF1 for various RNNa see Book, Chapter 6.**\n",
    "\n",
    "In general, there are **3 areas of interest**:\n",
    "* TensorFlow **RNN Cell** Classes: \n",
    "    * `tf.nn.rnn_cell`\n",
    "    * `tf.contrib.rnn` (in the zombie contrib module)\n",
    "* TensorFlow **RNN Model Construction** Classes:\n",
    "    * The **static** RNN classes add unrolled cells for time steps **at the compile time**, \n",
    "    * while **dynamic** RNN classes add unrolled cells for time steps **at the run time**.\n",
    "    * These are found:\n",
    "        * `tf.nn.static_rnn`\n",
    "        * `tf.nn.static_state_saving_rnn`\n",
    "        * `tf.nn.static_bidirectional_rnn`\n",
    "        * `tf.nn.dynamic_rnn`\n",
    "        * `tf.nn.bidirectional_dynamic_rnn`\n",
    "        * `tf.nn.raw_rnn`\n",
    "        * `tf.contrib.rnn.stack_bidirectional_dynamic_rnn`\n",
    "* TensorFlow RNN Cell **Wrapper** Classes:\n",
    "    * These **wrap other *cell* classes**.\n",
    "    * Found at:\n",
    "        * `tf.contrib.rnn.LSTMBlockWrapper`\n",
    "        * `tf.contrib.rnn.DropoutWrapper`\n",
    "        * `tf.contrib.rnn.EmbeddingWrapper`\n",
    "        * `tf.contrib.rnn.InputProjectionWrapper`\n",
    "        * `tf.contrib.rnn.OutputProjectionWrapper`\n",
    "        * `tf.contrib.rnn.DeviceWrapper`\n",
    "        * `tf.contrib.rnn.ResidualWrapper`\n",
    "        \n",
    "### Keras for RNN\n",
    "\n",
    "* Keras offers both *functional* and *sequential* API for creating the recurrent networks. \n",
    "* To build the RNN model, you have to add layers from the `kera.layers.recurrent` module. \n",
    "* Keras provides the following kinds of recurrent layers in the `keras.layers.recurrent` module:\n",
    "    * SimpleRNN\n",
    "    * LSTM\n",
    "    * GRU\n",
    "\n",
    "\n",
    "**Stateful Models (Keras)**\n",
    "\n",
    "Keras recurrent layers also support RNN models that **save state *between the batches***. \n",
    "\n",
    "You can create a stateful RNN, LSTM, or GRU model by passing `stateful` parameters as `True`. For stateful models, the batch size specified for the inputs *has to be a fixed value*. In stateful models, the hidden state learnt from training a batch is *reused for the next batch*. \n",
    "\n",
    "If you want to reset the memory at some point during training, it can be done with extra code by calling the `model.reset_states()` or `layer.reset_states()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although RNN is mostly used for sequence data, it can also be used for image data. \n",
    "\n",
    "We know that images have minimum two dimensions - height and width. Now think of one of the dimensions as time steps, and other as features. \n",
    "\n",
    "For MNIST, the image size is 28 x 28 pixels, thus **we can think of an MNIST image as having 28 time steps with 28 features in each timestep**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.18.5\n",
      "TensorFlow:1.15.5\n",
      "Keras:2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "print(\"Keras:{}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSLIB_HOME = '../datasetslib'\n",
    "\n",
    "import sys\n",
    "if not DATASETSLIB_HOME in sys.path:\n",
    "    sys.path.append(DATASETSLIB_HOME)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datasetslib\n",
    "\n",
    "datasetslib.datasets_root = os.path.join('../datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-0bec87c03a05>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../datasets/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\n",
    "    os.path.join(datasetslib.datasets_root, 'mnist'), \n",
    "    one_hot=True\n",
    ")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "Y_train = mnist.train.labels\n",
    "Y_test = mnist.test.labels\n",
    "\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_train): <class 'numpy.ndarray'>\n",
      "X_train.shape: (55000, 784)\n",
      "type(Y_train): <class 'numpy.ndarray'>\n",
      "Y_train.shape: (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(X_train):\", type(X_train))\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"type(Y_train):\", type(Y_train))\n",
    "print(\"Y_train.shape:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (55000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 28, 28)\n",
    "print(\"X_train.shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN With `Keras` for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import SimpleRNN    # Note the importing of SimpleRNN. ‚ö†Ô∏è `keras.layers.recurrent` doesn't exist anymore, just `keras.layers` now.\n",
    "from tensorflow.keras.optimizers import RMSprop  # Note the use of RMSprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the SimpleRNN model.\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=16, activation='relu', input_shape=(28,28)))\n",
    "# ^ Note:\n",
    "# From https://keras.io/api/layers/recurrent_layers/simple_rnn/#simplernn-layer\n",
    "# inputs: A 3D tensor, with shape [batch, timesteps, feature].\n",
    "model.add(Dense(n_classes))\n",
    "# ^ presumably this is applied to the RNN output from the final step. \n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 16)                720       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 890\n",
      "Trainable params: 890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(lr=0.01),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 1.2530 - acc: 0.5458\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.8466 - acc: 0.7054\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.7634 - acc: 0.7423\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.7215 - acc: 0.7636\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.6960 - acc: 0.7768\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.6645 - acc: 0.7851\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.6499 - acc: 0.7935\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.6259 - acc: 0.8029\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.6002 - acc: 0.8159\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5894 - acc: 0.8211\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5662 - acc: 0.8276\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5595 - acc: 0.8313\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.5644 - acc: 0.8308\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5651 - acc: 0.8301\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.5435 - acc: 0.8388\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5340 - acc: 0.8418\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.5335 - acc: 0.8434\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5311 - acc: 0.8455\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5161 - acc: 0.8479\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5184 - acc: 0.8473\n",
      "10000/10000 [==============================] - 1s 71us/sample - loss: 0.5016 - acc: 0.8495\n",
      "\n",
      "Test loss: 0.5015630922555924\n",
      "Test accuracy: 0.8495\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
