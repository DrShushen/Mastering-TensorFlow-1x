{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "<img src=\"https://keras.io/img/logo.png\" width=300 />\n",
    "\n",
    "### Currnet documentation\n",
    "1. Getting started: https://keras.io/getting_started/\n",
    "2. Guides: https://keras.io/guides/\n",
    "3. API reference: https://keras.io/api/\n",
    "\n",
    "### Notes:\n",
    "* TensorFlow team has included Keras in TensorFlow Core as module `tf.keras`.\n",
    "\n",
    "### Workflow\n",
    "1. Create the model\n",
    "2. Create and add layers to the model\n",
    "3. Compile the model\n",
    "4. Train the model\n",
    "5. Use the model for prediction or evaluation\n",
    "\n",
    "### Sequential vs Functional API\n",
    "The models in Keras can be created using the **sequential** or the **functional** APIs. \n",
    "Both the functional and sequential APIs can be used to build *any kind of models*. \n",
    "\n",
    "The **functional** API makes it **easier** to build the **complex models** that have multiple inputs, multiple outputs and shared layers.\n",
    "\n",
    "We have also observed that building simple models with the functional API makes it easier to grow the models into complex models with branching and sharing. Hence for our work, we always use the functional API.\n",
    "\n",
    "### Sequential API\n",
    "```python\n",
    "model = Sequential()\n",
    "# then .add() ...\n",
    "\n",
    "# Or, pass all layers to constructor as a list:\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(10, input_shape=(256,)),\n",
    "        Activation('tanh'),\n",
    "        Dense(10),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Functional API\n",
    "> In the functional API, you create the model as an instance of the `Model` class that takes an **input** and **output** parameter.\n",
    "\n",
    "The input and output parameters represent one or more input and output tensors, respectively.\n",
    "\n",
    "```python\n",
    "model = Model(inputs=tensor1, outputs=tensor2)\n",
    "```\n",
    "In the above code, `tensor1` and `tensor2` are either tensors **or objects that can be treated like tensors**, for example, Keras layer objects.\n",
    "\n",
    "\n",
    "If there are >1 input/output tensors, can pass as list:\n",
    "```python\n",
    "model = Model(inputs=[i1,i2,i3], outputs=[o1,o2,o3])\n",
    "```\n",
    "\n",
    "### Keras Layers\n",
    "* For an overview (as of *Keras 2*), see book ðŸ“š.\n",
    "* For up to date docs, see current docs.\n",
    "\n",
    "### Compiling the Model\n",
    "Signature of `model.compile()`:\n",
    "\n",
    "```python\n",
    "compile(\n",
    "    self, \n",
    "    optimizer, \n",
    "    loss, \n",
    "    metrics=None, \n",
    "    sample_weight_mode=None\n",
    ")\n",
    "```\n",
    "\n",
    "Where:\n",
    "* `optimizer` - own, or built-ins:\n",
    "    * SGD\n",
    "    * RMSprop\n",
    "    * Adagrad\n",
    "    * Adadelta\n",
    "    * Adam\n",
    "    * Adamax\n",
    "    * Nadam\n",
    "\n",
    "\n",
    "* `loss` - own or built-ins:\n",
    "    * mean_squared_error\n",
    "    * mean_absolute_error\n",
    "    * mean_absolute_pecentage_error\n",
    "    * mean_squared_logarithmic_error\n",
    "    * squared_hinge\n",
    "    * hinge\n",
    "    * categorical_hinge\n",
    "    * sparse_categorical_crossentropy\n",
    "    * binary_crossentropy\n",
    "    * poisson\n",
    "    * cosine proximity\n",
    "    * binary_accuracy\n",
    "    * categorical_accuracy\n",
    "    * sparse_categorical_accuracy\n",
    "    * top_k_categorical_accuracy\n",
    "    * sparse_top_k_categorical_accuracy\n",
    "\n",
    "\n",
    "* `metrics`: \n",
    "The third argument is a list of metrics that need to be collected while training the model. \n",
    "If verbose output is on, then the metrics are printed for each iteration. \n",
    "The metrics are like loss functions; some are provided by Keras with the ability to write your own metrics functions. \n",
    "All the loss functions also work as the metric function.\n",
    "\n",
    "### Training\n",
    "Signature of `model.fit()`:\n",
    "```python\n",
    "fit(\n",
    "    self, \n",
    "    x, \n",
    "    y, \n",
    "    batch_size=32, \n",
    "    epochs=10, \n",
    "    verbose=1, \n",
    "    callbacks=None,\n",
    "    validation_split=0.0, \n",
    "    validation_data=None, \n",
    "    shuffle=True,\n",
    "    class_weight=None, \n",
    "    sample_weight=None, \n",
    "    initial_epoch=0\n",
    ")\n",
    "```\n",
    "\n",
    "### Predicting\n",
    "The trained model can be used:\n",
    "* either to predict the value with the `model.predict()` method, \n",
    "* or to evaluate the model with the `model.evaluate()` method.\n",
    "\n",
    "Signatures:\n",
    "```python\n",
    "predict(self, x, batch_size=32, verbose=0)\n",
    "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "```\n",
    "\n",
    "### Additional Modules\n",
    "* The `preprocessing` module provides several functions for the preprocessing of sequence, image, and text data.\n",
    "* The `datasets` module provides several functions for quick access to several popular datasets, such as CIFAR10 images, CIFAR100 images, IMDB movie reviews, Reuters newswire topics, MNIST handwritten digits, and Boston housing prices.\n",
    "* The `initializers` module provides several functions to set initial random weight parameters of layers, such as `Zeros`, `Ones`, `Constant`, `RandomNormal`, `RandomUniform`, `TruncatedNormal`, `VarianceScaling`, `Orthogonal`, `Identity`, `lecun_normal`, `lecun_uniform`, `glorot_normal`, `glorot_uniform`, `he_normal`, and `he_uniform`.\n",
    "* The `models` module provides several functions to restore the model architectures and weights, such as `model_from_json`, `model_from_yaml`, and `load_model`. The model architectures can be saved using the `model.to_yaml()` and `model.to_json()` methods. The model weights can be saved by calling the `model.save()` method. **The weights get saved in an HDF5 file.** \n",
    "* The `applications` module provides several pre-built and pre-trained models such as Xception, VGG16, VGG19, ResNet50, Inception V3, InceptionResNet V2, and MobileNet. We shall learn how to use the pre-built models to predict with our datasets. We shall also learn how to retrain the pre-trained models in the applications module with our datasets from a slightly different domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras MNIST Example (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the keras modules\n",
    "\n",
    "# Use tensorflow.keras as keras  so that we don't have to install it separately.\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist  # <-- Keras' own MNIST dataset.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyper parameters\n",
    "batch_size = 100\n",
    "n_inputs = 784\n",
    "n_classes = 10\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the two dimensional 28 x 28 pixels sized images into a single vector of 784 pixels.\n",
    "x_train = x_train.reshape(60000, n_inputs)\n",
    "x_test = x_test.reshape(10000, n_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Convert the input values to float32\n",
    "print(x_train.dtype)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize the values of image vectors to fit under 1.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# convert output data into one hot encoded format\n",
    "print(y_train.shape)\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py37_tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Build a sequential model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# The first layer has to specify the dimensions of the input vector\n",
    "model.add(Dense(units=128, activation='sigmoid', input_shape=(n_inputs,)))\n",
    "\n",
    "# Add dropout layer for preventing overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=128, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer can only have the neurons equal to the number of outputs\n",
    "model.add(Dense(units=n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the summary of our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model!\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=SGD(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 2.3116 - acc: 0.1234\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 2.2380 - acc: 0.1916\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 2.1516 - acc: 0.2857\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 2.0153 - acc: 0.3988\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.8145 - acc: 0.4947\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.5735 - acc: 0.5673\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.3504 - acc: 0.6266\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 1.1739 - acc: 0.6700\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 1.0399 - acc: 0.7047\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.9397 - acc: 0.7315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5195317dd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.8175 - acc: 0.8047\n",
      "\n",
      " loss: 0.8174687757492065\n",
      "\n",
      " accuracy: 0.8047\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print the accuracy score\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('\\n loss:', scores[0])\n",
    "print('\\n accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Example in Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyper parameters\n",
    "batch_size = 100\n",
    "n_inputs = 784\n",
    "n_classes = 10\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the two dimensional 28 x 28 pixels sized images into a single vector of 784 pixels.\n",
    "x_train = x_train.reshape(60000, n_inputs)\n",
    "x_test = x_test.reshape(10000, n_inputs)\n",
    "\n",
    "# Convert the input values to float32\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "# Normalize the values of image vectors to fit under 1.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert output data into one hot encoded format\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports needed:\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model - functional\n",
    "\n",
    "input_ = Input(shape=(n_inputs,))\n",
    "\n",
    "# Build up the model layers in a functional manner:\n",
    "hidden = Dense(units=128)(input_)\n",
    "hidden = Activation(\"sigmoid\")(hidden)  # In this example, use activations explicitly with Activation.\n",
    "hidden = Dropout(0.1)(hidden)\n",
    "hidden = Dense(units=128)(hidden)\n",
    "hidden = Activation(\"sigmoid\")(hidden)\n",
    "hidden = Dropout(0.1)(hidden)\n",
    "\n",
    "output = Dense(units=n_classes)(hidden)\n",
    "output = Activation(\"softmax\")(output)\n",
    "\n",
    "# Define `Model`:\n",
    "model = Model(inputs=input_, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the summary of our model\n",
    "\n",
    "# Note that the summary isn't identical to the Sequential case, though the model is.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model!\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=SGD(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 2.3086 - acc: 0.1224\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 2.2438 - acc: 0.1844\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 2.1603 - acc: 0.2813\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 2.0333 - acc: 0.3855\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.8444 - acc: 0.4742\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.6157 - acc: 0.5462\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.4043 - acc: 0.6030\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 1.2322 - acc: 0.6489\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 1.0949 - acc: 0.6888\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.9828 - acc: 0.7207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5196ea7dd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.8596 - acc: 0.7932\n",
      "\n",
      " loss: 0.8595962315559387\n",
      "\n",
      " accuracy: 0.7932\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print the accuracy score\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('\\n loss:', scores[0])\n",
    "print('\\n accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
