{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Get-an-Interactive-TensorFlow-Session\" data-toc-modified-id=\"Get-an-Interactive-TensorFlow-Session-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Get an Interactive TensorFlow Session</a></span></li><li><span><a href=\"#Customary-Hello-TensorFlow-!!!\" data-toc-modified-id=\"Customary-Hello-TensorFlow-!!!-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Customary Hello TensorFlow !!!</a></span></li><li><span><a href=\"#Constants\" data-toc-modified-id=\"Constants-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Constants</a></span></li><li><span><a href=\"#Operations\" data-toc-modified-id=\"Operations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Operations</a></span></li><li><span><a href=\"#Placeholders\" data-toc-modified-id=\"Placeholders-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Placeholders</a></span></li><li><span><a href=\"#Creating-Tensors-from-Existing-Objects\" data-toc-modified-id=\"Creating-Tensors-from-Existing-Objects-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Creating Tensors from Existing Objects</a></span><ul class=\"toc-item\"><li><span><a href=\"#0-Dimensional-Tensors-(Scalars)\" data-toc-modified-id=\"0-Dimensional-Tensors-(Scalars)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>0-Dimensional Tensors (Scalars)</a></span></li><li><span><a href=\"#1-Dimensional-Tensors-(Vectors)\" data-toc-modified-id=\"1-Dimensional-Tensors-(Vectors)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>1-Dimensional Tensors (Vectors)</a></span></li><li><span><a href=\"#2-Dimensional-Tensors-(Matrices)\" data-toc-modified-id=\"2-Dimensional-Tensors-(Matrices)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>2-Dimensional Tensors (Matrices)</a></span></li><li><span><a href=\"#3-Dimensional-Tensors\" data-toc-modified-id=\"3-Dimensional-Tensors-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>3-Dimensional Tensors</a></span></li></ul></li><li><span><a href=\"#Variables\" data-toc-modified-id=\"Variables-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Variables</a></span></li><li><span><a href=\"#Creating-Tensors-from-Library-Functions\" data-toc-modified-id=\"Creating-Tensors-from-Library-Functions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Creating Tensors from Library Functions</a></span></li><li><span><a href=\"#Close-the-interactive-session\" data-toc-modified-id=\"Close-the-interactive-session-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Close the interactive session</a></span></li><li><span><a href=\"#Computation-Graphs\" data-toc-modified-id=\"Computation-Graphs-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Computation Graphs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-and-Running-simple-computation-graph\" data-toc-modified-id=\"Building-and-Running-simple-computation-graph-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Building and Running simple computation graph</a></span></li><li><span><a href=\"#Graph-on-Compute-Devices\" data-toc-modified-id=\"Graph-on-Compute-Devices-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Graph on Compute Devices</a></span></li><li><span><a href=\"#Executing-Graph-g-as-Default\" data-toc-modified-id=\"Executing-Graph-g-as-Default-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Executing Graph g as Default</a></span></li></ul></li><li><span><a href=\"#TensorBoard\" data-toc-modified-id=\"TensorBoard-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>TensorBoard</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 101 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an Interactive TensorFlow Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfs = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customary Hello TensorFlow !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello TensorFlow !!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(\"Hello TensorFlow !!\")\n",
    "print(tfs.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.constant(\n",
    "    value,\n",
    "    dtype=None,\n",
    "    shape=None,\n",
    "    name='Const',\n",
    "    verify_shape=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 (x):  Tensor(\"x:0\", shape=(), dtype=int32)\n",
      "c2 (y):  Tensor(\"y:0\", shape=(), dtype=float32)\n",
      "c3 (z):  Tensor(\"z:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c1 = tf.constant(5, name='x')\n",
    "c2 = tf.constant(6.0, name='y')\n",
    "c3 = tf.constant(7.0, tf.float32, name='z')\n",
    "print('c1 (x): ', c1)\n",
    "print('c2 (y): ', c2)\n",
    "print('c3 (z): ', c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run([c1,c2,c3]) :  [5, 6.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "print('run([c1,c2,c3]) : ', tfs.run([c1, c2, c3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op1 :  Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "op2 :  Tensor(\"Mul:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "op1 = tf.add(c2, c3)\n",
    "op2 = tf.multiply(c2, c3)\n",
    "print('op1 : ', op1)\n",
    "print('op2 : ', op2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(op1) :  13.0\n",
      "run(op2) :  42.0\n"
     ]
    }
   ],
   "source": [
    "print('run(op1) : ', tfs.run(op1))\n",
    "print('run(op2) : ', tfs.run(op2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note type is still considered Tensor here (after doing ops on it)\n",
    "type(op1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfs.run(op1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.placeholder(\n",
    "    dtype,\n",
    "    shape=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 :  Tensor(\"Placeholder:0\", dtype=float32)\n",
      "p2 :  Tensor(\"Placeholder_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p1 = tf.placeholder(tf.float32)\n",
    "p2 = tf.placeholder(tf.float32)\n",
    "print('p1 : ', p1)\n",
    "print('p2 : ', p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "op4 = p1 * p2  # shorthand for tf.multiply(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(p1))\n",
    "print(type(op4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(op4, {p1: 2.0, p2: 3.0}) :  6.0\n"
     ]
    }
   ],
   "source": [
    "print('run(op4, {p1: 2.0, p2: 3.0}) : ', tfs.run(op4, {p1: 2.0, p2: 3.0}))  # <-- feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(op4, feed_dict = {p1: 3.0, p2: 4.0}) :  12.0\n"
     ]
    }
   ],
   "source": [
    "print('run(op4, feed_dict = {p1: 3.0, p2: 4.0}) : ', \n",
    "      tfs.run(op4, feed_dict={p1: 3.0, p2: 4.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(op4, feed_dict={p1: [2.0,3.0,4.0], p2: [3.0,4.0,5.0]}): [ 6. 12. 20.]\n"
     ]
    }
   ],
   "source": [
    "print('run(op4, feed_dict={p1: [2.0,3.0,4.0], p2: [3.0,4.0,5.0]}):',\n",
    "      tfs.run(op4, feed_dict={p1: [2.0, 3.0, 4.0], p2: [3.0, 4.0, 5.0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ Note above that the Placeholder doesn't *necessarily* define a shape - we can potentially feed different shape tensors into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors from Existing Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.convert_to_tensor(\n",
    "    value,\n",
    "    dtype=None,\n",
    "    name=None,\n",
    "    preferred_dtype=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-Dimensional Tensors (Scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_t :  Tensor(\"Const_1:0\", shape=(), dtype=float64)\n",
      "run(tf_t) : \n",
      " 5.0\n"
     ]
    }
   ],
   "source": [
    "tf_t = tf.convert_to_tensor(5.0, dtype=tf.float64)\n",
    "\n",
    "print('tf_t : ', tf_t)\n",
    "print('run(tf_t) : \\n', tfs.run(tf_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Dimensional Tensors (Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1dim Shape :  (5,)\n",
      "tf_t :  Tensor(\"Const_2:0\", shape=(5,), dtype=float64)\n",
      "tf_t.shape:  (5,)\n",
      "tf_t[0] :  Tensor(\"strided_slice:0\", shape=(), dtype=float64)\n",
      "tf_t[0] :  Tensor(\"strided_slice_1:0\", shape=(), dtype=float64)\n",
      "run(tf_t) : \n",
      " [1.   2.   3.   4.   5.99]\n"
     ]
    }
   ],
   "source": [
    "a1dim = np.array([1, 2, 3, 4, 5.99])\n",
    "print(\"a1dim Shape : \", a1dim.shape)\n",
    "\n",
    "tf_t = tf.convert_to_tensor(a1dim, dtype=tf.float64)\n",
    "\n",
    "print('tf_t : ', tf_t)\n",
    "print('tf_t.shape: ', tf_t.shape)\n",
    "print('tf_t[0] : ', tf_t[0])\n",
    "print('tf_t[0] : ', tf_t[2])\n",
    "print('run(tf_t) : \\n', tfs.run(tf_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Dimensional Tensors (Matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2dim Shape :  (3, 5)\n",
      "tf_t :  Tensor(\"Const_3:0\", shape=(3, 5), dtype=float64)\n",
      "tf_t.shape :  (3, 5)\n",
      "tf_t[0][0] :  Tensor(\"strided_slice_3:0\", shape=(), dtype=float64)\n",
      "tf_t[1][2] :  Tensor(\"strided_slice_5:0\", shape=(), dtype=float64)\n",
      "run(tf_t) : \n",
      " [[1.   2.   3.   4.   5.99]\n",
      " [2.   3.   4.   5.   6.99]\n",
      " [3.   4.   5.   6.   7.99]]\n"
     ]
    }
   ],
   "source": [
    "a2dim = np.array(\n",
    "    [\n",
    "        (1, 2, 3, 4, 5.99),\n",
    "        (2, 3, 4, 5, 6.99),\n",
    "        (3, 4, 5, 6, 7.99)\n",
    "    ]\n",
    ")\n",
    "print(\"a2dim Shape : \", a2dim.shape)\n",
    "\n",
    "tf_t = tf.convert_to_tensor(a2dim, dtype=tf.float64)\n",
    "\n",
    "print('tf_t : ', tf_t)\n",
    "print('tf_t.shape : ', tf_t.shape)\n",
    "print('tf_t[0][0] : ', tf_t[0][0])\n",
    "print('tf_t[1][2] : ', tf_t[1][2])\n",
    "print('run(tf_t) : \\n', tfs.run(tf_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Dimensional Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3dim Shape :  (2, 2, 2)\n",
      "tf_t :  Tensor(\"Const_4:0\", shape=(2, 2, 2), dtype=float64)\n",
      "tf_t.shape :  (2, 2, 2)\n",
      "tf_t[0][0][0] :  Tensor(\"strided_slice_8:0\", shape=(), dtype=float64)\n",
      "tf_t[1][1][1] :  Tensor(\"strided_slice_11:0\", shape=(), dtype=float64)\n",
      "run(tf_t) : \n",
      " [[[1. 2.]\n",
      "  [3. 4.]]\n",
      "\n",
      " [[5. 6.]\n",
      "  [7. 8.]]]\n"
     ]
    }
   ],
   "source": [
    "a3dim = np.array(\n",
    "    [\n",
    "        [\n",
    "            [1, 2],\n",
    "            [3, 4],\n",
    "        ],\n",
    "        [\n",
    "            [5, 6],\n",
    "            [7, 8],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(\"a3dim Shape : \", a3dim.shape)\n",
    "\n",
    "tf_t = tf.convert_to_tensor(a3dim, dtype=tf.float64)\n",
    "\n",
    "print('tf_t : ', tf_t)\n",
    "print('tf_t.shape : ', tf_t.shape)\n",
    "print('tf_t[0][0][0] : ', tf_t[0][0][0])\n",
    "print('tf_t[1][1][1] : ', tf_t[1][1][1])\n",
    "print('run(tf_t) : \\n', tfs.run(tf_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.placeholder` defines input data that does not change over time. \n",
    "* `tf.Variable` defines variable values that are modified over time.\n",
    "* `tf.placeholder` does not need an initial value at the time of definition.\n",
    "* `tf.Variable` needs an initial value at the time of definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: <tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref>\n",
      "x: Tensor(\"Placeholder_2:0\", dtype=float32)\n",
      "b: <tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref>\n",
      "y: Tensor(\"add_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Assume Linear Model y = w * x + b\n",
    "# Define model parameters\n",
    "w = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "# Define model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = w * x + b\n",
    "\n",
    "print(\"w:\", w)\n",
    "print(\"x:\", x)\n",
    "print(\"b:\", b)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the \"_ref\" in dtype of Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(y, {x: [1,2,3,4]}) :  [0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "# initialize and print the variable y\n",
    "tf.global_variables_initializer().run()  # Alternatively: tfs.run( tf.global_variables_initializer() )\n",
    "print('run(y, {x: [1,2,3,4]}) : ', tfs.run(y, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the tf.variables_initializer() function to initialize only a set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors from Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros((100,))\n",
    "print(tfs.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"b:0\", shape=(2, 4), dtype=float32)\n",
      "[[7. 7. 7. 7.]\n",
      " [7. 7. 7. 7.]]\n"
     ]
    }
   ],
   "source": [
    "b = tf.fill((2, 4), value=7., name=\"b\")\n",
    "print(b)\n",
    "print(tfs.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"range:0\", shape=(9,), dtype=int32)\n",
      "[ 1 11 21 31 41 51 61 71 81]\n"
     ]
    }
   ],
   "source": [
    "r = tf.range(1, 91, delta=10)\n",
    "print(r)\n",
    "print(tfs.run(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"n:0\", shape=(3, 2), dtype=float64)\n",
      "[[-1.36682311  2.06684499]\n",
      " [-0.18542505 -3.43506363]\n",
      " [-0.44697834  0.39600509]]\n"
     ]
    }
   ],
   "source": [
    "# From distribution.\n",
    "tf.set_random_seed(12345)\n",
    "n = tf.random_normal(\n",
    "    (3, 2), \n",
    "    mean=0, \n",
    "    stddev=3.0, \n",
    "    dtype=tf.float64, \n",
    "    seed=777,  # NOTE THIS SEED HERE! \n",
    "    name=\"n\"\n",
    ")\n",
    "\n",
    "print(n)\n",
    "print(tfs.run(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Random Seeds in TF1\n",
    "The distributions generated are affected by the **graph-level** or the **operation-level** seed. \n",
    "\n",
    "* The **graph-level seed** is set using `tf.set_random_seed()`.\n",
    "* \\[⚠️\\] The **operation-level seed** is given *as the argument seed in **all of the random distribution functions***. \n",
    "\n",
    "If no seed is specified, then a random seed is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `tf.get_variable()` returns the existing variable with the same name if it exists, and creates the variable with the specified shape and initializer if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable(name='w', dtype=tf.float32, initializer=[.3])\n",
    "b = tf.get_variable(name='b', dtype=tf.float32, initializer=[-.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛑 **Info in the book here is out of date!**\n",
    "\n",
    "Read this article instead: https://medium.com/@hideyuki/what-does-variable-reuse-mean-in-tensorflow-40e86535026b\n",
    "\n",
    "Covers `scope`, `reuse`, `AUTO_REUSE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚠️ See my notebook: [my-nb-01_Variable_Reuse.ipynb](./my-nb-01_Variable_Reuse.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close the interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A computation graph is made up of **nodes** and **edges**. Each node represents an operation (`tf.Operation`) and each edge represents a tensor (`tf.Tensor`) that gets transferred between the nodes.\n",
    "\n",
    "A program in TensorFlow is basically a computation graph. \n",
    "\n",
    "You create the graph with nodes representing:\n",
    "* variables, \n",
    "* constants, \n",
    "* placeholders, \n",
    "* and operations \n",
    "\n",
    "and feed it to TensorFlow. \n",
    "\n",
    "TensorFlow finds the first nodes that it can fire or execute. The firing of these nodes results in the firing of other nodes, and so on.\n",
    "\n",
    "Thus, TensorFlow programs are made up of two kinds of operations on computation graphs:\n",
    "* Building the computation graph\n",
    "* Running the computation graph\n",
    "\n",
    "<br/>\n",
    "\n",
    "TensorFlow comes with a **default graph**. Unless another graph is explicitly specified, *a new node gets implicitly added to the default graph*. We can get explicit access to the default graph using the following command: `graph = tf.get_default_graph()`\n",
    "\n",
    "For example, if we want to define three inputs and add them to produce output, we can represent it using the following computation graph:\n",
    "\n",
    "![ComputationGraph](./ComputationGraph.png)\n",
    "\n",
    "In TensorFlow, the add operation in the preceding image would correspond to the code `y = tf.add( x1 + x2 + x3 )`.\n",
    "\n",
    "As we create the variables, constants, and placeholders, they get added to the graph. Then we create a **session** object to execute the operation objects and evaluate the tensor objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Running simple computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  [0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "# Assume Linear Model y = w * x + b\n",
    "# Define model parameters\n",
    "w = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "# Define model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = w * x + b\n",
    "\n",
    "with tf.Session() as tfs:  # NOTE: the with block will automatically call tfs.close() at the end.\n",
    "    \n",
    "    # initialize and print the variable y\n",
    "    tf.global_variables_initializer().run()\n",
    "    output = tfs.run(y, feed_dict={x: [1, 2, 3, 4]})\n",
    "\n",
    "    print('output : ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order of execution and lazy loading\n",
    "\n",
    "The nodes are executed in the order of dependency. If node $a$ depends on node $b$, then $a$ will be executed before $b$ when the execution of $b$ is requested. \n",
    "\n",
    "A node is not executed unless either the node itself or another node depending on it is not requested for execution. This is\n",
    "also known as lazy loading; namely, the node objects are not created and initialized until they are needed.\n",
    "\n",
    "Sometimes, you may want to control the order in which the nodes are executed in a graph. This can be achieved with the `tf.Graph.control_dependencies()` function. For example, if the graph has nodes $a$, $b$, $c$, and $d$ and you want to execute $c$ and $d$ before $a$ and $b$, then use the following statement:\n",
    "```python\n",
    "with graph_variable.control_dependencies([c, d]):\n",
    "    # other statements here\n",
    "```\n",
    "This makes sure that any node in the preceding with block is executed only after nodes $c$ and $d$ have been executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph on Compute Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17202171288944793001\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6020470525130642112\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18132847180197464680\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 89653248\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17621415796009797239\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In their case, the output was:\n",
    "```python\n",
    "[name: \"/cpu:0\"\n",
    "device_type: \"CPU\"\n",
    "memory_limit: 268435456\n",
    "locality {\n",
    "}\n",
    "incarnation: 12445270569278384213\n",
    ", name: \"/gpu:0\"\n",
    "device_type: \"GPU\"\n",
    "memory_limit: 25628672\n",
    "locality {\n",
    "  bus_id: 1\n",
    "}\n",
    "incarnation: 5284662930836416221\n",
    "physical_device_desc: \"device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The devices in TensorFlow are identified with the string `/device:<device_type>:<device_idx>` . \n",
    "\n",
    "One thing to note about the above output is that it shows only one CPU, whereas our computer has 8 CPUs. The reason for that is TensorFlow implicitly distributes the code across the CPU units and thus by default `CPU:0` denotes all the CPU's available to TensorFlow. When TensorFlow starts executing graphs, it runs the independent paths within each graph in a separate thread, with each thread running on a separate CPU. \n",
    "\n",
    "We can restrict the number of threads used for this purpose by changing the number of `inter_op_parallelism_threads`. Similarly, if within an independent path, an operation is capable of running on multiple threads, TensorFlow will launch that specific operation on multiple threads. The number of threads in this pool can be changed by setting the number of `intra_op_parallelism_threads`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing graph nodes on *specific compute devices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "output [0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define model parameters\n",
    "w = tf.get_variable(name='w', initializer=[.3], dtype=tf.float32)\n",
    "b = tf.get_variable(name='b', initializer=[-.3], dtype=tf.float32)\n",
    "# Define model input and output\n",
    "x = tf.placeholder(name='x', dtype=tf.float32)\n",
    "y = w * x + b\n",
    "\n",
    "# Enable LOGGING of device placement.\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(config=config) as tfs:\n",
    "    # initialize and print the variable y\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    print('output', tfs.run(y, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Confirmed on CPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the device placement, look at the TERMINAL (not printed within jupyter)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "output [0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/device:CPU:0'):  # <--- `tf.device()` context manager.\n",
    "    # Define model parameters\n",
    "    w = tf.get_variable(name='w', initializer=[.3], dtype=tf.float32)\n",
    "    b = tf.get_variable(name='b', initializer=[-.3], dtype=tf.float32)\n",
    "    # Define model input and output\n",
    "    x = tf.placeholder(name='x', dtype=tf.float32)\n",
    "    y = w * x + b\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(config=config) as tfs:\n",
    "    # initialize and print the variable y\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    print('output', tfs.run(y, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Confirmed on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "output [0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    # Define model parameters\n",
    "    w = tf.get_variable(name='w', initializer=[.3], dtype=tf.float32)\n",
    "    b = tf.get_variable(name='b', initializer=[-.3], dtype=tf.float32)\n",
    "    # Define model input and output\n",
    "    x = tf.placeholder(name='x', dtype=tf.float32)\n",
    "\n",
    "with tf.device('/device:GPU:0'):  # <-- Place y (which comes from w, x and b) on a DIFFERENT device!!!\n",
    "    y = w * x + b\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(config=config) as tfs:\n",
    "    # initialize and print the variable y\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    print('output', tfs.run(y, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Confirmed some on CPU then on GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple placement\n",
    "TensorFlow follows these simple rules, also known as the simple placement, for placing the variables on the devices:\n",
    "```bash\n",
    "If the graph was previously run,\n",
    "    then the node is left on the device where it was placed earlier\n",
    "Else If the tf.device() block is used,\n",
    "    then the node is placed on the specified device\n",
    "Else If the GPU is present\n",
    "    then the node is placed on the first available GPU\n",
    "Else If the GPU is not present\n",
    "    then the node is placed on the CPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic placement\n",
    "The `tf.device()` can also be **passed a function name** instead of a device string. \n",
    "\n",
    "In such case, the function must return the device string. \n",
    "\n",
    "This feature allows complex algorithms for placing the variables on different devices. For example, TensorFlow provides a round robin device setter in `tf.train.replica_device_setter()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft placement\n",
    "When you place a TensorFlow operation on the GPU, the TF must have the GPU implementation of that operation, known as the *kernel*. If the kernel is not present then the placement results in run-time error. Also if the GPU device you requested does not exist, you will get a run-time error. The best way to handle such errors is to allow the operation to be placed on the CPU if requesting the GPU device results in n error. This can be achieved by setting the following config value:\n",
    "```python\n",
    "config.allow_soft_placement = True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU memory handling\n",
    "\n",
    "When you start running the TensorFlow session, by default it grabs all of the GPU memory, even if you place the operations and variables only on one GPU in a multi-GPU system. If you try to run another session at the same time, you will get out of memory error. This can be solved in multiple ways:\n",
    "\n",
    "* For multi-GPU systems, set the environment variable `CUDA_VISIBLE_DEVICES=<list of device idx>`\n",
    "```python\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "```\n",
    "The code executed after this setting will be able to grab all of the memory of only the visible GPU.\n",
    "\n",
    "* When you do not want the session to grab all of the memory of the GPU, then you can use the config option `per_process_gpu_memory_fraction` to allocate a percentage of memory:\n",
    "```python\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "```\n",
    "This will allocate 50% of the memory of all the GPU devices.\n",
    "\n",
    "* You can also combine both of the above strategies, i.e. make only a percentage along with making only some of the GPU visible to the process.\n",
    "\n",
    "* You can also limit the TensorFlow process to grab only the minimum required memory at the start of the process. As the process executes further, you can set a config option to allow the growth of this memory.\n",
    "```python\n",
    "config.gpu_options.allow_growth = True\n",
    "```\n",
    "This option only allows for the allocated memory to grow, **but the memory is never released back**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Graph g as Default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Multople Graphs\n",
    "You can create your own graphs separate from the default graph and execute them in a session. However, creating and **executing multiple graphs is not recommended**, as it has the following disadvantages:\n",
    "* Creating and using multiple graphs in the same program would require multiple TensorFlow sessions and each session would consume heavy resources\n",
    "* You cannot directly pass data in between graphs\n",
    "\n",
    "Hence, the recommended approach is to have **multiple *subgraphs*** in a single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  [0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "# Here is an example where we create our own graph, g , and execute it as the default graph:\n",
    "\n",
    "g = tf.Graph()  # <-- NOTE.\n",
    "\n",
    "with g.as_default():  # <-- NOTE.\n",
    "    w = tf.Variable([.3], tf.float32)\n",
    "    b = tf.Variable([-.3], tf.float32)\n",
    "    x = tf.placeholder(tf.float32)\n",
    "    y = w * x + b\n",
    "\n",
    "with tf.Session(graph=g) as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    output = tfs.run(y, {x: [1, 2, 3, 4]})\n",
    "\n",
    "print('output : ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TensorBoard:\n",
    "> 1. visualizes computation graph structure, \n",
    "> 2. provides statistical analysis, \n",
    "> 3. plots the values captured as summaries during the execution of computation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(y, feed_dict={x: 3} :  [0.6]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Assume Linear Model y = w * x + b\n",
    "\n",
    "# Define model parameters\n",
    "w = tf.Variable([.3], name='w', dtype=tf.float32)\n",
    "b = tf.Variable([-.3], name='b', dtype=tf.float32)\n",
    "\n",
    "# Define model input and output\n",
    "x = tf.placeholder(name='x', dtype=tf.float32)\n",
    "y = w * x + b\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    writer = tf.summary.FileWriter('tflogs', tfs.graph)  # <-- Create tf.summary.FileWriter that would create the output in the tflogs folder with the events from the default graph.\n",
    "    print('run(y, feed_dict={x: 3} : ', tfs.run(y, feed_dict={x: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### execute: `tensorboard --logdir='tflogs'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TensorBoardGraph](./TensorBoardGraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard workflow (advanced)\n",
    "\n",
    "TensorBoard works by reading log files generated by TensorFlow. Thus, we need to modify the programming model defined here to incorporate additional operation nodes that would produce the information in the logs that we want to visualize using TensorBoard. The programming model or the flow of programs with TensorBoard can be generally stated as follows:\n",
    "\n",
    "1. Create the computational graph as usual.\n",
    "\n",
    "2. Create summary nodes. Attach summary operations from the `tf.summary` package to the nodes that output the values that you wish to collect and analyze.\n",
    "\n",
    "3. Run the summary nodes along with running your model nodes. Generally, you would use the convenience function, `tf.summary.merge_all()`, to merge all the summary nodes into one summary node. Then executing this merged node would basically execute all the summary nodes. The merged summary node produces a serialized `Summary` ProtocolBuffers object containing the union of all the summaries.\n",
    "\n",
    "4. Write the event logs to disk by passing the `Summary` ProtocolBuffers object to a `tf.summary.FileWriter` object.\n",
    "\n",
    "5. Start TensorBoard and analyze the visualized data. \n",
    "\n",
    "**In the above section, we did not create summary nodes but used TensorBoard in a very simple way.** \n",
    "We will cover the advanced usage of TensorBoard later in this book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "nav_menu": {
    "height": "306px",
    "width": "240px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
